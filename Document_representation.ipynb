{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45c93016-1604-40a4-9283-068f3a51c35d",
   "metadata": {},
   "source": [
    "# Document representation using NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c149dc-8d3b-4632-bd18-5dbc70d280d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "![alt text](library-of-babel.jpg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ba8fa6-62ec-416d-a96b-e6a531049e22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "import re\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.cluster import KMeans\n",
    "from IPython.display import IFrame\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from scipy.spatial import distance\n",
    "from IPython.display import IFrame\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "from nltk import word_tokenize\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words('english'))\n",
    "import string\n",
    "punct = list(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce4ce23-1a06-44d2-b699-8cc65039823f",
   "metadata": {},
   "source": [
    "# 1. Distance metrics in document representation\n",
    "\n",
    "If we can represent documents as vectors, this means we can measure the similarity (or distance) between them using geometrical tools. This is because each vector allows us to think of a document as a point in space, where each number in the vector is a dimension. We have encountered this idea already in our exploration of VAD, where the valence-arousal-dominance score of a word or phrase allows us to plot that word or phrase in 3D space. Most of the time we deal in dimensions greater than three in NLP so we can't intuitively visualise the dimensionality of the space. Nevertheless, distance metrics from 2D and 3D space easily generalise. We typically use two distance metrics in NLP:\n",
    "\n",
    "### 1. Euclidean distance\n",
    "\n",
    "The Euclidean distance between two vectors is the length of the shortest line that joins their endpoints. For vectors $A$ and $B$ 2D space, it looks like this:\n",
    " \n",
    "<img src=\"euclidean.png\" alt=\"alt text\" width=\"300\" height=\"200\"/>\n",
    "\n",
    "For vectors $A(x_1,x_2,…,x_n)$ and $B(y_1,y_2,…,y_n)$ in $n$-dimensional space, the formula for Euclidean distance, $d$, is:\n",
    "\n",
    "$$d = \\sqrt{\\sum_{i=1}^{n} (y_i - x_i)^2}$$\n",
    "\n",
    "Note that Euclidean distance is always positive: there is no such thing as negative distance. As there <i>is</i> such a thing as opposite meaning, this can mean that it is not an ideal metric for measuring document meaning.\n",
    "\n",
    "## 2. Cosine similarity:\n",
    "\n",
    "Cosine similarity is different from Euclidean distance in that it works with the <i>angles</i> between vectors. The idea is that two vectors that 'point' in the same direction are likely to be similar, and thus that any two documents that are representated by these vectors are also likely to be similar.\n",
    "\n",
    "<img src=\"cosine_2.png\" alt=\"alt text\" width=\"500\" height=\"400\"/>\n",
    "\n",
    "For vectors $A(x_1,x_2,…,x_n)$ and $B(y_1,y_2,…,y_n)$ in $n$-dimensional space, the formula for cosine similarity is given by:\n",
    "\n",
    "$$\n",
    "\\text{sim}(A,B) = \\frac{\\sum_{i=1}^n A_i B_i}{\\sqrt{\\sum_{i=1}^n A_i^2} \\times \\sqrt{\\sum_{i=1}^n B_i^2}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $\\sum_{i=1}^n A_i B_i$ is the dot product of $A$ and $B$,\n",
    "- $\\sqrt{\\sum_{i=1}^n A_i^2}$ is the magnitude of vector $A$,\n",
    "- $\\sqrt{\\sum_{i=1}^n B_i^2}$ is the magnitude of vector $B$.\n",
    "\n",
    "Unlike Euclidean distance, cosine similarity can take any value between $-1$ and $1$. Specifically:\n",
    "\n",
    "* If two documents have a score of $1$ the have the same meaning\n",
    "* If two documents have a score of $0$ they have unrelated meanings\n",
    "* If two documents have a score of $-1$ they have opposite meanings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db73df0-9984-4a70-a3b3-f57dc85e702e",
   "metadata": {},
   "source": [
    "### Two distance examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e440133-2179-49cc-ad3c-e91332c96209",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"punky hairstyle\"\n",
    "d2 = \"tangent to a point\"\n",
    "\n",
    "d1 = nlp(d1).vector\n",
    "d2 = nlp(d2).vector\n",
    "\n",
    "euc = distance.euclidean(d1, d2)\n",
    "cos = 1 - distance.cosine(d1, d2)\n",
    "\n",
    "print(f\"The Euclidean distance between the documents is {euc}.\")\n",
    "print(f\"The cosine similarity between the documents is {cos}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2ad21-727c-449e-a63a-17ca00638ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cos_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2c139-7ae1-49e6-aaf4-13ad580f3763",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Topic modelling with Topic-frequency, Inverse-document-frequency (TF-IDF)\n",
    "\n",
    "While it's relatively easy to group words together, matters are less simple for documents. Nevertheless, we are usually far more interested in grouping documents together than we are words, especially when they all belong to a corpus. So an early problem that emerged in NLP centred on how we might do this. \n",
    "\n",
    "One solution proposed by Karen Spärck Jones in 1972 was TF-IDF scoring. This method is based in two ideas:\n",
    "\n",
    "* We should 'punish' words that occur frequently in all the documents in our corpus\n",
    "* We should 'reward' words that occur frequently in small numbers of documents in our corpus\n",
    "\n",
    "The intuition behind these ideas is fairly simple. For a topic to be coherent, it will generally consist of a small number of concepts. As these concepts will expressed as words, we should expect these topic-related words to concentrate in the documents that belong to this topic. However, some words (like prepositions and common verbs) will appear in all documents. So if we can can create a word-document matrix that assigns a score to each word in each document, we can gain a numerical representation––a vector––of how each document in a corpus differs from every other one.\n",
    "\n",
    "Once we have these vectors, we can then perform various operations (like clustering) upon them to discover how they might be grouped together. But how do we calculate them?\n",
    "\n",
    "The TF-IDF score for a word is the product of two quantities: the term frequency, and the inverse document frequency.\n",
    "\n",
    "$$TF\\text{-}IDF (t, d, D) = tf(t,d)\\times idf(t, D)$$\n",
    "\n",
    "Where $t$ is a term, $d$ is a document, and $D$ is the corpus. Let's create a toy corpus to illustrate this.\n",
    "\n",
    "$D$ = {{$d_1$: Atomic Burger makes a tasty burger}, {$d_2$: An atomic clock is accurate}, {$d_3$: Atomic weapons are destructive}}\n",
    "\n",
    "First, let's look at how we might calculate $tf(t,d)$. This is the relative frequency of each term in each document. That is, it's the number of times a term $t_i$ occurs in a document $d$ divided by the total number of terms in the document:\n",
    "\n",
    "$$tf(t,d) = \\frac{f_{t_i,d}}{\\sum\\limits_{t\\in d}f_{t,d}}$$\n",
    "\n",
    "For example, in document $d_1$ 'burger' has a $tf(t,d)$ score of 0.333 because there are six words and it occurs twice. Every other word has a score of 0.16 because it only occurs once. Words that occur often are therefore 'rewarded' in this part of the calculation.\n",
    "\n",
    "Now, let's look at $idf(t,D)$. This adjusts the $tf(t,d)$ score by capturing how often a word occurs across the entire corpus of documents. It is the logarithm of the number of documents in a corpus divided by the number of douments that contain the term in the corpus. \n",
    "\n",
    "$$idf(t,D) = \\log{\\frac{N}{|\\{d\\in D: t \\in d\\}|}}$$\n",
    "\n",
    "What's happening here? If a term occurs in all documents, the formula outputs a value of 0, because the total number of documents divided by the number of dcouments containing the term is $\\log(1)= 0$. If the term occurs in a smaller number of documents, the formula gives a larger number, with the largest number being given when the term only occurs in one document. For example, 'atomic' occurs in all documents, so the $idf(t,D)$ value for 'atomic' is 0. However, 'burger' occurs in only one document, so the value is $\\log{\\frac{3}{1}} = 1.09$. In this way, the $idf(t,D)$ 'punishes' words that are common and therefore not topic specific. \n",
    "\n",
    "The result is that by multiplying $tf(t,d)$ and $idf(t,D)$, we are able to capture the role played by a word in determining the topic of a document relative to a corpus. The $TF\\text{-}IDF (t, d, D)$ representation of a document is a vector of the values taken by all the words in that document relative to all the words in the corpus (with a value of 0 being taken when a word does not appear in the document)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8582f8-dfc9-4eb4-9fca-6989d627cdd9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input = 'content', strip_accents = 'ascii', stop_words = 'english')\n",
    "\n",
    "D = ['Atomic Burger makes a tasty burger', 'An atomic clock is accurate', 'Atomic weapons are destructive']\n",
    "\n",
    "v = vectorizer.fit_transform(D)\n",
    "v = v.todense().tolist()\n",
    "\n",
    "d = pd.DataFrame(\n",
    "    v,columns=vectorizer.get_feature_names_out())\n",
    "d.index = ['d1', 'd2', 'd3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9c023-01c9-4984-be7a-7e5c54ce5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "cos = [[] for i in range(len(d))]\n",
    "\n",
    "for i in range(len(d)):\n",
    "    for j in range(len(d)):\n",
    "        cos[i].append(distance.cosine(d.iloc[i], d.iloc[j]))\n",
    "        \n",
    "cos_df = pd.DataFrame(cos, columns = d.index, index = d.index)\n",
    "\n",
    "euc = [[] for i in range(len(d))]\n",
    "\n",
    "for i in range(len(d)):\n",
    "    for j in range(len(d)):\n",
    "        euc[i].append(distance.euclidean(d.iloc[i], d.iloc[j]))\n",
    "        \n",
    "euc_df = pd.DataFrame(euc, columns = d.index, index = d.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0bb07-fcb4-4bd3-974f-575d08b73560",
   "metadata": {},
   "source": [
    "# 3. A real-world example of TF-IDF\n",
    "\n",
    "We're going to look at a Twitter dataset that contains tweets about eating disorders. The idea is to create a Tf-IDF representation of each tweet and see of we can find any patterns in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91213cc3-68c2-4917-9bf9-31b3bf3e722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load some data\n",
    "\n",
    "data_ed = pd.read_excel('ED_twitter_data.xlsx', index_col = 0)\n",
    "data_ed = data_ed.drop_duplicates(subset = ['tweet']).reset_index(drop = True)\n",
    "data_ed.rename(columns={'tweet':'text'}, inplace=True)\n",
    "data_ed['source'] = 'Eating disorder'\n",
    "\n",
    "\n",
    "data_random = pd.read_csv('twitter_gender_processed.csv')\n",
    "data_random['source'] = 'Random'\n",
    "\n",
    "data_ed = data_ed[['text', 'source']]\n",
    "data_random = data_random[['text', 'source']]\n",
    "\n",
    "data_random = data_random.sample(n = len(data_ed))\n",
    "\n",
    "data = pd.concat([data_ed, data_random]).reset_index(drop = True)\n",
    "\n",
    "                                 \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2fb98d-0086-499e-adf4-b11382abbd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d69eae-0087-43c2-83f2-4a3f43828fa4",
   "metadata": {},
   "source": [
    "### Creating a tokenizer \n",
    "\n",
    "First, let's construct a tokenizer that turns our data into lemmas and passes them to the TF-IDF vectorizer we want to use. We're doing this for two reasons:\n",
    "\n",
    "* The `scikit-learn` tokenizer that comes with the TF-IDF library isn't very good; we've already seen that stopwords and redundant variation introduce a lot of noise into the analysis.\n",
    "* We'd like <i>in general</i> to be able to tweak our code to reflect what interests us, so it's important to know how to do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362c0470-3491-4020-8ac1-4dc592f6e6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A good tokenizer:\n",
    "\n",
    "def good_tokens(text):\n",
    "    words = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(i).lower() for i in words]\n",
    "    lemmas = [i for i in lemmas if i not in stops and i not in punct]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851bf474-7010-4c6e-85e1-ad004eb03f33",
   "metadata": {},
   "source": [
    "### Break the text into a list of documents (our corpus) and create the TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa8ec8-d3f8-4b8b-b9d7-5070cff919d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [str(i) for i in data['text']]\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=good_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33423993-2d5d-41a8-95f0-f9e5bf5feb7d",
   "metadata": {},
   "source": [
    "### Now create a TF-IDF representation of our corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28545ea-fb48-4d7f-b2cb-fe0da2447a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = vectorizer.fit_transform(texts)\n",
    "vectors = vectors.todense().tolist()\n",
    "\n",
    "df = pd.DataFrame(vectors,columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4030453c-c434-47ee-bd0a-745475f6973c",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53b639b-aaa9-42d7-8370-cad295734f00",
   "metadata": {},
   "source": [
    "### How can we visualise this high dimensional data? \n",
    "\n",
    "There are a few ways in which relations between data points in high dimensions can be made visible. A very common one is known as <i>principal components analysis</i>. This works by finding common patterns of variation between dimensions and collapsing them into each other. The problem with this is that squashing a high dimensional space into a low dimensional space means lots of detail is lost. However, in can be a useful guide for identifying patterns. All calculations should be performed on the original TF-IDF vectors, not the PCA reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f610f7c-c732-43d2-b9de-a059ee2bfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_1 = PCA(n_components = 3)\n",
    "comps_1 = pca_1.fit_transform(df)\n",
    "pc_df_1 = pd.DataFrame(data = comps_1, columns = ['PC'+str(i) for i in range(1, comps_1.shape[1]+1)])\n",
    "pc_df_1['tweet text'] = data['text']\n",
    "\n",
    "\n",
    "fig = px.scatter_3d(pc_df_1, x='PC1', y='PC2', z='PC3', hover_data = ['tweet text'])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceedc09-d95a-4055-ae4d-78812fbda64a",
   "metadata": {},
   "source": [
    "### How can we cluster our data using a distance metric?\n",
    "\n",
    "There are several options when it comes to dividing our data into clusters. Usually, we need to know how many clusters we want in advance. Kmeans clustering works by taking $n$ clusters and finding the grouping the data that minimises the variance withing each cluster, wkere $n$ is set by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264193ae-d661-490b-a8f8-42c575c3b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0, n_init=\"auto\").fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde70a8f-4c7a-4750-a58d-3367fe952285",
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_df_1['clusters'] = [str(i) for i in kmeans.labels_]\n",
    "pc_df_1['source'] = data['source']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfedf98-2c6a-436c-86cc-234bf01cfa44",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(pc_df_1, x='PC1', y='PC2', z='PC3', color = 'clusters', hover_data = ['source', 'tweet text'])\n",
    "\n",
    "fig.update_traces(marker=dict(size = 5, line=dict(width=2,\n",
    "                                        color='DarkSlateGrey')),\n",
    "                  selector=dict(mode='markers'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb40d137-19ff-489f-a1d3-3afe06f4d9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
